{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30775,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import torch\nfrom torch.utils.data import DataLoader\nfrom transformers import BertForSequenceClassification, BertTokenizer, AdamW, get_linear_schedule_with_warmup\nfrom sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score, roc_auc_score\nimport pandas as pd\nimport numpy as np\nfrom transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-10-03T17:44:22.267094Z","iopub.execute_input":"2024-10-03T17:44:22.267544Z","iopub.status.idle":"2024-10-03T17:44:53.300429Z","shell.execute_reply.started":"2024-10-03T17:44:22.267495Z","shell.execute_reply":"2024-10-03T17:44:53.299587Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"# Load Train, Validation, Test Dataset\ntrain_dataset = pd.read_csv('/kaggle/input/mp-data/MP_IN_adm_train.csv')\nval_dataset = pd.read_csv('/kaggle/input/mp-data/MP_IN_adm_val.csv')\ntest_dataset = pd.read_csv('/kaggle/input/mp-data/MP_IN_adm_test.csv')","metadata":{"execution":{"iopub.status.busy":"2024-10-03T17:44:53.302457Z","iopub.execute_input":"2024-10-03T17:44:53.303101Z","iopub.status.idle":"2024-10-03T17:44:55.561249Z","shell.execute_reply.started":"2024-10-03T17:44:53.303065Z","shell.execute_reply":"2024-10-03T17:44:55.560373Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"from torch.utils.data import DataLoader\nfrom torch import nn\n\nclass EnsembleModel(nn.Module):\n    def __init__(self, model1):\n        super(EnsembleModel, self).__init__()\n        self.model1 = model1\n\n    def forward(self, input_ids, attention_mask):\n        output1 = self.model1(input_ids, attention_mask=attention_mask)[0]\n        avg_output = output1\n        return avg_output","metadata":{"execution":{"iopub.status.busy":"2024-10-03T17:44:55.562416Z","iopub.execute_input":"2024-10-03T17:44:55.562734Z","iopub.status.idle":"2024-10-03T17:44:55.569336Z","shell.execute_reply.started":"2024-10-03T17:44:55.562699Z","shell.execute_reply":"2024-10-03T17:44:55.568356Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"\nfrom transformers import DistilBertForSequenceClassification, DistilBertConfig, AdamW, get_linear_schedule_with_warmup\n\n# create a student model\nstudent_config = DistilBertConfig.from_pretrained('distilbert-base-uncased', \n                                                  num_labels=2, \n                                                  hidden_dropout_prob=0.2, \n                                                  attention_probs_dropout_prob=0.2)\n\nstudent_model = DistilBertForSequenceClassification(student_config)\n\n# set the temperature\ntemperature = 2.0","metadata":{"execution":{"iopub.status.busy":"2024-10-03T17:44:55.570672Z","iopub.execute_input":"2024-10-03T17:44:55.571045Z","iopub.status.idle":"2024-10-03T17:44:57.053277Z","shell.execute_reply.started":"2024-10-03T17:44:55.570999Z","shell.execute_reply":"2024-10-03T17:44:57.052329Z"},"trusted":true},"execution_count":4,"outputs":[{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/483 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1c6b083971074a9980810fd5dee295cf"}},"metadata":{}}]},{"cell_type":"code","source":"from transformers import AutoTokenizer\n\n# Choose a tokenizer\ntokenizer = AutoTokenizer.from_pretrained('distilbert-base-uncased')","metadata":{"execution":{"iopub.status.busy":"2024-10-03T17:44:57.055989Z","iopub.execute_input":"2024-10-03T17:44:57.056355Z","iopub.status.idle":"2024-10-03T17:44:57.959343Z","shell.execute_reply.started":"2024-10-03T17:44:57.056308Z","shell.execute_reply":"2024-10-03T17:44:57.958313Z"},"trusted":true},"execution_count":5,"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"90e4a6cabede43158758bf7226c2c284"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"713e1787c3a6468396260066ee8a7888"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"78adc2a12e264666baf8755fed0063c5"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n  warnings.warn(\n","output_type":"stream"}]},{"cell_type":"code","source":"# Apply the tokenizer to the training, validation, and test datasets\ntrain_encodings = tokenizer(train_dataset['text'].tolist(), truncation=True, padding=True, max_length = 512)\nval_encodings = tokenizer(val_dataset['text'].tolist(), truncation=True, padding=True,  max_length = 512)\ntest_encodings = tokenizer(test_dataset['text'].tolist(), truncation=True, padding=True , max_length = 512)","metadata":{"execution":{"iopub.status.busy":"2024-10-03T17:44:57.960994Z","iopub.execute_input":"2024-10-03T17:44:57.961394Z","iopub.status.idle":"2024-10-03T17:45:39.066149Z","shell.execute_reply.started":"2024-10-03T17:44:57.961341Z","shell.execute_reply":"2024-10-03T17:45:39.065324Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"# Create a Dataset for PyTorch\nclass LosDataset(torch.utils.data.Dataset):\n    def __init__(self, encodings, labels):\n        self.encodings = encodings\n        self.labels = labels\n\n    def __getitem__(self, idx):\n        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n        item['labels'] = torch.tensor(self.labels[idx])\n        return item\n\n    def __len__(self):\n        return len(self.labels)","metadata":{"execution":{"iopub.status.busy":"2024-10-03T17:45:39.067332Z","iopub.execute_input":"2024-10-03T17:45:39.067668Z","iopub.status.idle":"2024-10-03T17:45:39.074331Z","shell.execute_reply.started":"2024-10-03T17:45:39.067633Z","shell.execute_reply":"2024-10-03T17:45:39.073477Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"train_dataset = LosDataset(train_encodings, train_dataset['hospital_expire_flag'].tolist())\nval_dataset = LosDataset(val_encodings, val_dataset['hospital_expire_flag'].tolist())\ntest_dataset = LosDataset(test_encodings, test_dataset['hospital_expire_flag'].tolist())","metadata":{"execution":{"iopub.status.busy":"2024-10-03T17:45:39.075728Z","iopub.execute_input":"2024-10-03T17:45:39.076036Z","iopub.status.idle":"2024-10-03T17:45:39.092495Z","shell.execute_reply.started":"2024-10-03T17:45:39.076002Z","shell.execute_reply":"2024-10-03T17:45:39.091818Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"from transformers import AdamW, get_linear_schedule_with_warmup\nfrom sklearn.metrics import precision_score, recall_score, f1_score, roc_auc_score, accuracy_score\nfrom tqdm import tqdm\nfrom torch import nn\nimport numpy as np\n\n# Create the ensemble model\nensemble_model = EnsembleModel(student_model)","metadata":{"execution":{"iopub.status.busy":"2024-10-03T17:45:39.093602Z","iopub.execute_input":"2024-10-03T17:45:39.093859Z","iopub.status.idle":"2024-10-03T17:45:39.106725Z","shell.execute_reply.started":"2024-10-03T17:45:39.093829Z","shell.execute_reply":"2024-10-03T17:45:39.105910Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"import os\n\n# list all files in the current directory\nfiles = os.listdir('.')\n\n# filter the ones that start with 'CORE_baseline'\ncore_models = [f for f in files if f.startswith('distilBERT_baseline_MP')]\n\nif core_models:\n    print(\"Found models starting with 'distilBERT_baseline_MP':\")\n    for model in core_models:\n        print(model)\n        \n    # get the first (and supposedly only) model\n    model_path = core_models[0]\n\n    # load the model state\n    ensemble_model.load_state_dict(torch.load(model_path))\n    print(\"Loaded Model\")\nelse:\n    print(\"No models found starting with 'distilBERT_baseline_MP'.\")","metadata":{"execution":{"iopub.status.busy":"2024-10-03T17:45:39.107755Z","iopub.execute_input":"2024-10-03T17:45:39.108026Z","iopub.status.idle":"2024-10-03T17:45:39.117632Z","shell.execute_reply.started":"2024-10-03T17:45:39.107995Z","shell.execute_reply":"2024-10-03T17:45:39.116704Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stdout","text":"No models found starting with 'distilBERT_baseline_MP'.\n","output_type":"stream"}]},{"cell_type":"code","source":"# Push the model to device\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nensemble_model = ensemble_model.to(device)","metadata":{"execution":{"iopub.status.busy":"2024-10-03T17:45:39.118810Z","iopub.execute_input":"2024-10-03T17:45:39.119219Z","iopub.status.idle":"2024-10-03T17:45:39.489013Z","shell.execute_reply.started":"2024-10-03T17:45:39.119170Z","shell.execute_reply":"2024-10-03T17:45:39.488113Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"ensemble_model","metadata":{"execution":{"iopub.status.busy":"2024-10-03T17:45:39.490191Z","iopub.execute_input":"2024-10-03T17:45:39.490590Z","iopub.status.idle":"2024-10-03T17:45:39.498220Z","shell.execute_reply.started":"2024-10-03T17:45:39.490524Z","shell.execute_reply":"2024-10-03T17:45:39.497311Z"},"trusted":true},"execution_count":12,"outputs":[{"execution_count":12,"output_type":"execute_result","data":{"text/plain":"EnsembleModel(\n  (model1): DistilBertForSequenceClassification(\n    (distilbert): DistilBertModel(\n      (embeddings): Embeddings(\n        (word_embeddings): Embedding(30522, 768, padding_idx=0)\n        (position_embeddings): Embedding(512, 768)\n        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n        (dropout): Dropout(p=0.1, inplace=False)\n      )\n      (transformer): Transformer(\n        (layer): ModuleList(\n          (0-5): 6 x TransformerBlock(\n            (attention): MultiHeadSelfAttention(\n              (dropout): Dropout(p=0.1, inplace=False)\n              (q_lin): Linear(in_features=768, out_features=768, bias=True)\n              (k_lin): Linear(in_features=768, out_features=768, bias=True)\n              (v_lin): Linear(in_features=768, out_features=768, bias=True)\n              (out_lin): Linear(in_features=768, out_features=768, bias=True)\n            )\n            (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (ffn): FFN(\n              (dropout): Dropout(p=0.1, inplace=False)\n              (lin1): Linear(in_features=768, out_features=3072, bias=True)\n              (lin2): Linear(in_features=3072, out_features=768, bias=True)\n              (activation): GELUActivation()\n            )\n            (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n          )\n        )\n      )\n    )\n    (pre_classifier): Linear(in_features=768, out_features=768, bias=True)\n    (classifier): Linear(in_features=768, out_features=2, bias=True)\n    (dropout): Dropout(p=0.2, inplace=False)\n  )\n)"},"metadata":{}}]},{"cell_type":"code","source":"train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\nval_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\ntest_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)","metadata":{"execution":{"iopub.status.busy":"2024-10-03T17:45:39.499486Z","iopub.execute_input":"2024-10-03T17:45:39.499850Z","iopub.status.idle":"2024-10-03T17:45:39.509199Z","shell.execute_reply.started":"2024-10-03T17:45:39.499812Z","shell.execute_reply":"2024-10-03T17:45:39.508271Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"epochs = 200\nbest_roc_auc = 0.0\nmin_delta = 0.0001\nearly_stopping_count = 0\nearly_stopping_patience = 3\ngradient_accumulation_steps = 10\nbest_model_path = \"best_model.pth\"\n\n# Set the optimizer\noptimizer = AdamW(ensemble_model.parameters(), lr=1e-5, weight_decay=0.01)\n\n# Set the scheduler\nscheduler = get_linear_schedule_with_warmup(\n    optimizer, \n    num_warmup_steps=50, \n    num_training_steps=len(train_loader) * epochs // gradient_accumulation_steps\n)","metadata":{"execution":{"iopub.status.busy":"2024-10-03T17:45:39.513086Z","iopub.execute_input":"2024-10-03T17:45:39.513752Z","iopub.status.idle":"2024-10-03T17:45:40.174115Z","shell.execute_reply.started":"2024-10-03T17:45:39.513717Z","shell.execute_reply":"2024-10-03T17:45:40.173331Z"},"trusted":true},"execution_count":14,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n  warnings.warn(\n","output_type":"stream"}]},{"cell_type":"code","source":"from torch.nn import functional as F\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n\n# Training\nfor epoch in range(epochs):\n    ensemble_model.train()\n    train_loss = 0\n    for step, batch in enumerate(tqdm(train_loader)):\n        optimizer.zero_grad() if step % gradient_accumulation_steps == 0 else None\n        input_ids = batch['input_ids'].to(device)\n        attention_mask = batch['attention_mask'].to(device)\n        labels = batch['labels'].to(device)\n        outputs = ensemble_model(input_ids, attention_mask)\n        loss = nn.CrossEntropyLoss()(outputs, labels)\n        (loss / gradient_accumulation_steps).backward()\n        train_loss += loss.item()\n        if (step + 1) % gradient_accumulation_steps == 0 or (step + 1) == len(train_loader):\n            optimizer.step()\n            scheduler.step()\n\n    ensemble_model.eval()\n    val_loss = 0\n    val_preds = []\n    val_labels = []\n    with torch.no_grad():\n        for batch in tqdm(val_loader):\n            input_ids = batch['input_ids'].to(device)\n            attention_mask = batch['attention_mask'].to(device)\n            labels = batch['labels'].to(device)\n            outputs = ensemble_model(input_ids, attention_mask)\n            loss = nn.CrossEntropyLoss()(outputs, labels)\n            val_loss += loss.item()\n            val_preds.append(F.softmax(outputs, dim=1).cpu().numpy())\n            val_labels.append(labels.cpu().numpy())\n            \n    val_preds = np.concatenate(val_preds)\n    val_labels = np.concatenate(val_labels)\n    val_loss /= len(val_loader)\n    train_loss /= len(train_loader)\n    print(f'Epoch: {epoch+1}/{epochs}, Training Loss: {train_loss}, Validation Loss: {val_loss}')\n\n    # Calculate metrics\n    val_preds_class = np.argmax(val_preds, axis=1)\n    accuracy = accuracy_score(val_labels, val_preds_class)\n    recall = recall_score(val_labels, val_preds_class)\n    precision = precision_score(val_labels, val_preds_class)\n    f1 = f1_score(val_labels, val_preds_class)\n    roc_auc = roc_auc_score(val_labels, val_preds[:, 1])\n\n    print(f'Accuracy: {accuracy}, Recall: {recall}, Precision: {precision}, F1: {f1}, Roc Auc: {roc_auc}')\n\n    # Implement early stopping\n    if epoch > 0 and roc_auc - best_roc_auc < min_delta:\n        early_stopping_count += 1\n        print(f'EarlyStopping counter: {early_stopping_count} out of {early_stopping_patience}')\n        if early_stopping_count >= early_stopping_patience:\n            print('Early stopping')\n            break\n    else:\n        best_roc_auc = roc_auc\n        early_stopping_count = 0\n        torch.save(ensemble_model.state_dict(), f\"distilBERT_baseline_MP_epoch_{epoch}roc_{best_roc_auc}.pth\")","metadata":{"execution":{"iopub.status.busy":"2024-10-03T17:45:40.175302Z","iopub.execute_input":"2024-10-03T17:45:40.175625Z","iopub.status.idle":"2024-10-04T00:11:20.215147Z","shell.execute_reply.started":"2024-10-03T17:45:40.175591Z","shell.execute_reply":"2024-10-04T00:11:20.214135Z"},"trusted":true},"execution_count":15,"outputs":[{"name":"stderr","text":"100%|██████████| 1062/1062 [26:02<00:00,  1.47s/it]\n100%|██████████| 154/154 [01:23<00:00,  1.85it/s]\n/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n","output_type":"stream"},{"name":"stdout","text":"Epoch: 1/200, Training Loss: 0.42624114210000036, Validation Loss: 0.33751997061364064\nAccuracy: 0.8944580277098615, Recall: 0.0, Precision: 0.0, F1: 0.0, Roc Auc: 0.5982352837705913\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 1062/1062 [26:12<00:00,  1.48s/it]\n100%|██████████| 154/154 [01:23<00:00,  1.85it/s]\n/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n","output_type":"stream"},{"name":"stdout","text":"Epoch: 2/200, Training Loss: 0.334605549183269, Validation Loss: 0.3351525234711635\nAccuracy: 0.8944580277098615, Recall: 0.0, Precision: 0.0, F1: 0.0, Roc Auc: 0.6079348466592203\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 1062/1062 [26:11<00:00,  1.48s/it]\n100%|██████████| 154/154 [01:23<00:00,  1.85it/s]\n/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n","output_type":"stream"},{"name":"stdout","text":"Epoch: 3/200, Training Loss: 0.33269857362670413, Validation Loss: 0.33211792957086067\nAccuracy: 0.8944580277098615, Recall: 0.0, Precision: 0.0, F1: 0.0, Roc Auc: 0.6218821734197588\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 1062/1062 [26:11<00:00,  1.48s/it]\n100%|██████████| 154/154 [01:23<00:00,  1.85it/s]\n/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n","output_type":"stream"},{"name":"stdout","text":"Epoch: 4/200, Training Loss: 0.3268627272725891, Validation Loss: 0.3229933969386212\nAccuracy: 0.8944580277098615, Recall: 0.0, Precision: 0.0, F1: 0.0, Roc Auc: 0.6680446961768145\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 1062/1062 [26:10<00:00,  1.48s/it]\n100%|██████████| 154/154 [01:23<00:00,  1.85it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch: 5/200, Training Loss: 0.3072468488777088, Validation Loss: 0.2941317301388685\nAccuracy: 0.8946617766911166, Recall: 0.0019305019305019305, Precision: 1.0, F1: 0.0038535645472061652, Roc Auc: 0.7607756308211889\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 1062/1062 [26:10<00:00,  1.48s/it]\n100%|██████████| 154/154 [01:22<00:00,  1.86it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch: 6/200, Training Loss: 0.28646372637506257, Validation Loss: 0.2839455353942784\nAccuracy: 0.8962917685411573, Recall: 0.08108108108108109, Precision: 0.56, F1: 0.14165261382799327, Roc Auc: 0.7903716765903553\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 1062/1062 [26:09<00:00,  1.48s/it]\n100%|██████████| 154/154 [01:23<00:00,  1.85it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch: 7/200, Training Loss: 0.268555588679995, Validation Loss: 0.2779343267539879\nAccuracy: 0.8952730236348818, Recall: 0.11196911196911197, Precision: 0.5178571428571429, F1: 0.18412698412698414, Roc Auc: 0.8041138600364113\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 1062/1062 [26:08<00:00,  1.48s/it]\n100%|██████████| 154/154 [01:23<00:00,  1.85it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch: 8/200, Training Loss: 0.2568367277035269, Validation Loss: 0.3053064735896595\nAccuracy: 0.8973105134474327, Recall: 0.055984555984555984, Precision: 0.6590909090909091, F1: 0.10320284697508897, Roc Auc: 0.8100698322793995\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 1062/1062 [26:09<00:00,  1.48s/it]\n100%|██████████| 154/154 [01:22<00:00,  1.86it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch: 9/200, Training Loss: 0.24933251646643753, Validation Loss: 0.2743537996883516\nAccuracy: 0.8930317848410758, Recall: 0.17567567567567569, Precision: 0.48148148148148145, F1: 0.25742574257425743, Roc Auc: 0.8129128151907196\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 1062/1062 [26:09<00:00,  1.48s/it]\n100%|██████████| 154/154 [01:23<00:00,  1.85it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch: 10/200, Training Loss: 0.23952282347633957, Validation Loss: 0.28026703372597694\nAccuracy: 0.8854930725346374, Recall: 0.2606177606177606, Precision: 0.4299363057324841, F1: 0.3245192307692307, Roc Auc: 0.8151137632914398\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 1062/1062 [26:06<00:00,  1.47s/it]\n100%|██████████| 154/154 [01:23<00:00,  1.85it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch: 11/200, Training Loss: 0.2349550521085407, Validation Loss: 0.3038657027018535\nAccuracy: 0.8985330073349633, Recall: 0.0945945945945946, Precision: 0.6282051282051282, F1: 0.16442953020134227, Roc Auc: 0.8162729439494816\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 1062/1062 [26:09<00:00,  1.48s/it]\n100%|██████████| 154/154 [01:23<00:00,  1.85it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch: 12/200, Training Loss: 0.2294359154781772, Validation Loss: 0.28470618031048156\nAccuracy: 0.8983292583537082, Recall: 0.12548262548262548, Precision: 0.5855855855855856, F1: 0.20667726550079493, Roc Auc: 0.8151625755270401\nEarlyStopping counter: 1 out of 3\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 1062/1062 [26:08<00:00,  1.48s/it]\n100%|██████████| 154/154 [01:23<00:00,  1.85it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch: 13/200, Training Loss: 0.22029451837828085, Validation Loss: 0.2952255007822986\nAccuracy: 0.8936430317848411, Recall: 0.16988416988416988, Precision: 0.4888888888888889, F1: 0.25214899713467054, Roc Auc: 0.8148402388721296\nEarlyStopping counter: 2 out of 3\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 1062/1062 [26:09<00:00,  1.48s/it]\n100%|██████████| 154/154 [01:23<00:00,  1.85it/s]","output_type":"stream"},{"name":"stdout","text":"Epoch: 14/200, Training Loss: 0.22063848987860138, Validation Loss: 0.2847086194322094\nAccuracy: 0.8909942950285249, Recall: 0.2084942084942085, Precision: 0.463519313304721, F1: 0.28761651131824234, Roc Auc: 0.8150420840625853\nEarlyStopping counter: 3 out of 3\nEarly stopping\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}]},{"cell_type":"code","source":"import os\n\n# list all files in the current directory\nfiles = os.listdir('.')\n\n# filter the ones that start with 'CORE_baseline'\ncore_models = [f for f in files if f.startswith('distilBERT_baseline_MP')]\n\nif core_models:\n    print(\"Found models starting with 'distilBERT_baseline_MP':\")\n    for model in core_models:\n        print(model)\n        \n    # get the first (and supposedly only) model\n    model_path = core_models[0]\n\n    # load the model state\n    ensemble_model.load_state_dict(torch.load(model_path))\n    print(\"Loaded Model\")\nelse:\n    print(\"No models found starting with 'distilBERT_baseline_MP'.\")","metadata":{"execution":{"iopub.status.busy":"2024-10-04T00:11:20.216826Z","iopub.execute_input":"2024-10-04T00:11:20.217198Z","iopub.status.idle":"2024-10-04T00:11:20.453329Z","shell.execute_reply.started":"2024-10-04T00:11:20.217160Z","shell.execute_reply":"2024-10-04T00:11:20.452439Z"},"trusted":true},"execution_count":16,"outputs":[{"name":"stderr","text":"/tmp/ipykernel_30/2612778148.py:18: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  ensemble_model.load_state_dict(torch.load(model_path))\n","output_type":"stream"},{"name":"stdout","text":"Found models starting with 'distilBERT_baseline_MP':\ndistilBERT_baseline_MP_epoch_3roc_0.6680446961768145.pth\ndistilBERT_baseline_MP_epoch_9roc_0.8151137632914398.pth\ndistilBERT_baseline_MP_epoch_4roc_0.7607756308211889.pth\ndistilBERT_baseline_MP_epoch_10roc_0.8162729439494816.pth\ndistilBERT_baseline_MP_epoch_5roc_0.7903716765903553.pth\ndistilBERT_baseline_MP_epoch_1roc_0.6079348466592203.pth\ndistilBERT_baseline_MP_epoch_2roc_0.6218821734197588.pth\ndistilBERT_baseline_MP_epoch_8roc_0.8129128151907196.pth\ndistilBERT_baseline_MP_epoch_6roc_0.8041138600364113.pth\ndistilBERT_baseline_MP_epoch_0roc_0.5982352837705913.pth\ndistilBERT_baseline_MP_epoch_7roc_0.8100698322793995.pth\nLoaded Model\n","output_type":"stream"}]},{"cell_type":"code","source":"from torch.nn import functional as F\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n\n# Put the model in evaluation mode\nensemble_model.eval()\n\n# Initialize lists to store predictions and true labels\ntest_preds = []\ntest_labels = []\n\n# Iterate over test data\nwith torch.no_grad():\n    for batch in tqdm(test_loader):\n        input_ids = batch['input_ids'].to(device)\n        attention_mask = batch['attention_mask'].to(device)\n        labels = batch['labels'].to(device)\n        outputs = ensemble_model(input_ids, attention_mask)\n        test_preds.append(F.softmax(outputs, dim=1).cpu().numpy())\n        test_labels.append(labels.cpu().numpy())","metadata":{"execution":{"iopub.status.busy":"2024-10-04T00:11:20.454631Z","iopub.execute_input":"2024-10-04T00:11:20.455012Z","iopub.status.idle":"2024-10-04T00:14:07.289033Z","shell.execute_reply.started":"2024-10-04T00:11:20.454977Z","shell.execute_reply":"2024-10-04T00:14:07.287981Z"},"trusted":true},"execution_count":17,"outputs":[{"name":"stderr","text":"100%|██████████| 307/307 [02:46<00:00,  1.84it/s]\n","output_type":"stream"}]},{"cell_type":"code","source":"test_preds = np.concatenate(test_preds)\ntest_labels = np.concatenate(test_labels)\n\n# Calculate metrics\ntest_preds_class = np.argmax(test_preds, axis=1)\naccuracy = accuracy_score(test_labels, test_preds_class)\nrecall = recall_score(test_labels, test_preds_class)\nprecision = precision_score(test_labels, test_preds_class)\nf1 = f1_score(test_labels, test_preds_class)\nroc_auc = roc_auc_score(test_labels, test_preds[:, 1])\n\nprint(f'Accuracy: {accuracy}, Recall: {recall}, Precision: {precision}, F1: {f1}, Roc Auc: {roc_auc}')","metadata":{"execution":{"iopub.status.busy":"2024-10-04T00:14:07.290623Z","iopub.execute_input":"2024-10-04T00:14:07.291005Z","iopub.status.idle":"2024-10-04T00:14:07.328499Z","shell.execute_reply.started":"2024-10-04T00:14:07.290968Z","shell.execute_reply":"2024-10-04T00:14:07.327377Z"},"trusted":true},"execution_count":18,"outputs":[{"name":"stdout","text":"Accuracy: 0.8956424353492161, Recall: 0.0, Precision: 0.0, F1: 0.0, Roc Auc: 0.6943097009235409\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n","output_type":"stream"}]}]}