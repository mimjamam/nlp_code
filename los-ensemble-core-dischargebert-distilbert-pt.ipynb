{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":9473241,"sourceType":"datasetVersion","datasetId":5761232}],"dockerImageVersionId":30787,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import torch\nfrom torch.utils.data import DataLoader\nfrom transformers import BertForSequenceClassification, BertTokenizer, AdamW, get_linear_schedule_with_warmup\nfrom sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score, roc_auc_score\nimport pandas as pd\nimport numpy as np\nfrom transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-10-05T14:56:24.588244Z","iopub.execute_input":"2024-10-05T14:56:24.588656Z","iopub.status.idle":"2024-10-05T14:56:43.543168Z","shell.execute_reply.started":"2024-10-05T14:56:24.588604Z","shell.execute_reply":"2024-10-05T14:56:43.542410Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"# Load Train, Validation, Test Dataset\ntrain_dataset = pd.read_csv('/kaggle/input/mydata/LOS_WEEKS_adm_train.csv')\nval_dataset = pd.read_csv('/kaggle/input/mydata/LOS_WEEKS_adm_val.csv')\ntest_dataset = pd.read_csv('/kaggle/input/mydata/LOS_WEEKS_adm_test.csv')","metadata":{"execution":{"iopub.status.busy":"2024-10-05T14:56:43.545067Z","iopub.execute_input":"2024-10-05T14:56:43.546081Z","iopub.status.idle":"2024-10-05T14:56:46.189733Z","shell.execute_reply.started":"2024-10-05T14:56:43.546034Z","shell.execute_reply":"2024-10-05T14:56:46.188894Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"from torch.utils.data import DataLoader\nfrom torch import nn\n\nclass EnsembleModel(nn.Module):\n    def __init__(self, model1, model2):\n        super(EnsembleModel, self).__init__()\n        self.model1 = model1\n        self.model2 = model2\n\n    def forward(self, input_ids, attention_mask):\n        output1 = self.model1(input_ids, attention_mask=attention_mask)[0]\n        output2 = self.model2(input_ids, attention_mask=attention_mask)[0]\n        avg_output = (output1 + output2) / 2.00\n        return avg_output","metadata":{"execution":{"iopub.status.busy":"2024-10-05T14:56:46.190907Z","iopub.execute_input":"2024-10-05T14:56:46.191182Z","iopub.status.idle":"2024-10-05T14:56:46.197772Z","shell.execute_reply.started":"2024-10-05T14:56:46.191153Z","shell.execute_reply":"2024-10-05T14:56:46.196900Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"from transformers import AutoModelForSequenceClassification, AutoConfig\n\n# Specify the dropout rate in the configuration\nconfig = AutoConfig.from_pretrained('bvanaken/CORe-clinical-outcome-biobert-v1', \n                                    num_labels=4, \n                                    hidden_dropout_prob=0.2, \n                                    attention_probs_dropout_prob=0.2)\n\n# Load the pre-trained model with the specified configuration\ncore_model = AutoModelForSequenceClassification.from_pretrained('bvanaken/CORe-clinical-outcome-biobert-v1', config=config)","metadata":{"execution":{"iopub.status.busy":"2024-10-05T14:56:46.199782Z","iopub.execute_input":"2024-10-05T14:56:46.200126Z","iopub.status.idle":"2024-10-05T14:56:49.330846Z","shell.execute_reply.started":"2024-10-05T14:56:46.200094Z","shell.execute_reply":"2024-10-05T14:56:49.329912Z"},"trusted":true},"execution_count":4,"outputs":[{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/428 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"aafb7f796d79487fb723a7097c28f4d1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"pytorch_model.bin:   0%|          | 0.00/433M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7dea33d965d34e868e5711cdda7ac9b8"}},"metadata":{}},{"name":"stderr","text":"Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bvanaken/CORe-clinical-outcome-biobert-v1 and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"}]},{"cell_type":"code","source":"from transformers import AutoModelForSequenceClassification, AutoConfig\n\n# Specify the dropout rate in the configuration\nconfig = AutoConfig.from_pretrained('emilyalsentzer/Bio_Discharge_Summary_BERT', \n                                    num_labels=4, \n                                    hidden_dropout_prob=0.2, \n                                    attention_probs_dropout_prob=0.2)\n\n# Load the pre-trained model with the specified configuration\ndischarge_model = AutoModelForSequenceClassification.from_pretrained('emilyalsentzer/Bio_Discharge_Summary_BERT', config=config)","metadata":{"execution":{"iopub.status.busy":"2024-10-05T14:56:49.332084Z","iopub.execute_input":"2024-10-05T14:56:49.332395Z","iopub.status.idle":"2024-10-05T14:56:51.796652Z","shell.execute_reply.started":"2024-10-05T14:56:49.332362Z","shell.execute_reply":"2024-10-05T14:56:51.795710Z"},"trusted":true},"execution_count":5,"outputs":[{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/385 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4d5500c5365841c9b36f2fd674988d36"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"pytorch_model.bin:   0%|          | 0.00/436M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f04bb68c8093422da9c4b482e2648ad2"}},"metadata":{}},{"name":"stderr","text":"Some weights of BertForSequenceClassification were not initialized from the model checkpoint at emilyalsentzer/Bio_Discharge_Summary_BERT and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"}]},{"cell_type":"code","source":"from transformers import AutoTokenizer\n\n# Choose a tokenizer\ntokenizer = AutoTokenizer.from_pretrained('bvanaken/CORe-clinical-outcome-biobert-v1')","metadata":{"execution":{"iopub.status.busy":"2024-10-05T14:56:51.797953Z","iopub.execute_input":"2024-10-05T14:56:51.798330Z","iopub.status.idle":"2024-10-05T14:56:52.601026Z","shell.execute_reply.started":"2024-10-05T14:56:51.798284Z","shell.execute_reply":"2024-10-05T14:56:52.600094Z"},"trusted":true},"execution_count":6,"outputs":[{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/213k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6c580395229943bb963fc2843a42136f"}},"metadata":{}}]},{"cell_type":"code","source":"# Apply the tokenizer to the training, validation, and test datasets\ntrain_encodings = tokenizer(train_dataset['text'].tolist(), truncation=True, padding=True, max_length = 512)\nval_encodings = tokenizer(val_dataset['text'].tolist(), truncation=True, padding=True,  max_length = 512)\ntest_encodings = tokenizer(test_dataset['text'].tolist(), truncation=True, padding=True , max_length = 512)","metadata":{"execution":{"iopub.status.busy":"2024-10-05T14:56:52.602147Z","iopub.execute_input":"2024-10-05T14:56:52.602461Z","iopub.status.idle":"2024-10-05T14:57:29.047332Z","shell.execute_reply.started":"2024-10-05T14:56:52.602422Z","shell.execute_reply":"2024-10-05T14:57:29.046520Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"# Create a Dataset for PyTorch\nclass LosDataset(torch.utils.data.Dataset):\n    def __init__(self, encodings, labels):\n        self.encodings = encodings\n        self.labels = labels\n\n    def __getitem__(self, idx):\n        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n        item['labels'] = torch.tensor(self.labels[idx])\n        return item\n\n    def __len__(self):\n        return len(self.labels)","metadata":{"execution":{"iopub.status.busy":"2024-10-05T14:57:29.048363Z","iopub.execute_input":"2024-10-05T14:57:29.048662Z","iopub.status.idle":"2024-10-05T14:57:29.055069Z","shell.execute_reply.started":"2024-10-05T14:57:29.048630Z","shell.execute_reply":"2024-10-05T14:57:29.054110Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"train_dataset = LosDataset(train_encodings, train_dataset['los_label'].tolist())\nval_dataset = LosDataset(val_encodings, val_dataset['los_label'].tolist())\ntest_dataset = LosDataset(test_encodings, test_dataset['los_label'].tolist())","metadata":{"execution":{"iopub.status.busy":"2024-10-05T14:57:29.056187Z","iopub.execute_input":"2024-10-05T14:57:29.056513Z","iopub.status.idle":"2024-10-05T14:57:29.075925Z","shell.execute_reply.started":"2024-10-05T14:57:29.056481Z","shell.execute_reply":"2024-10-05T14:57:29.075078Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"from transformers import AdamW, get_linear_schedule_with_warmup\nfrom sklearn.metrics import precision_score, recall_score, f1_score, roc_auc_score, accuracy_score\nfrom tqdm import tqdm\nfrom torch import nn\nimport numpy as np\n\n# Create the ensemble model\nensemble_model = EnsembleModel(core_model, discharge_model)","metadata":{"execution":{"iopub.status.busy":"2024-10-05T14:57:29.079002Z","iopub.execute_input":"2024-10-05T14:57:29.079356Z","iopub.status.idle":"2024-10-05T14:57:29.086224Z","shell.execute_reply.started":"2024-10-05T14:57:29.079325Z","shell.execute_reply":"2024-10-05T14:57:29.085354Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"import os\n\n# list all files in the current directory\nfiles = os.listdir('.')\n\n# filter the ones that start with 'CORE_baseline'\ncore_models = [f for f in files if f.startswith('CORE_ensemble(core + dischargebert)')]\n\nif core_models:\n    print(\"Found models starting with 'CORE_ensemble(core + dischargebert)':\")\n    for model in core_models:\n        print(model)\n        \n    # get the first (and supposedly only) model\n    model_path = core_models[0]\n\n    # load the model state\n    ensemble_model.load_state_dict(torch.load(model_path))\n    print(f\"Loaded Model: {model_path}\")\nelse:\n    print(\"No models found starting with 'CORE_ensemble(core + dischargebert)'.\")","metadata":{"execution":{"iopub.status.busy":"2024-10-05T14:57:29.087255Z","iopub.execute_input":"2024-10-05T14:57:29.087568Z","iopub.status.idle":"2024-10-05T14:57:29.098742Z","shell.execute_reply.started":"2024-10-05T14:57:29.087520Z","shell.execute_reply":"2024-10-05T14:57:29.097904Z"},"trusted":true},"execution_count":11,"outputs":[{"name":"stdout","text":"No models found starting with 'CORE_ensemble(core + dischargebert)'.\n","output_type":"stream"}]},{"cell_type":"code","source":"# Push the model to device\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nensemble_model = ensemble_model.to(device)","metadata":{"execution":{"iopub.status.busy":"2024-10-05T14:57:29.099737Z","iopub.execute_input":"2024-10-05T14:57:29.100009Z","iopub.status.idle":"2024-10-05T14:57:29.583729Z","shell.execute_reply.started":"2024-10-05T14:57:29.099980Z","shell.execute_reply":"2024-10-05T14:57:29.582669Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"from transformers import DistilBertForSequenceClassification, DistilBertConfig, AdamW, get_linear_schedule_with_warmup\n\n# create a student model\nstudent_config = DistilBertConfig.from_pretrained('distilbert-base-uncased', \n                                                  num_labels=4, \n                                                  hidden_dropout_prob=0.2, \n                                                  attention_probs_dropout_prob=0.2)\n\nstudent_model = DistilBertForSequenceClassification(student_config)\n\n# set the temperature\ntemperature = 2.0","metadata":{"execution":{"iopub.status.busy":"2024-10-05T14:57:29.584826Z","iopub.execute_input":"2024-10-05T14:57:29.585148Z","iopub.status.idle":"2024-10-05T14:57:30.828388Z","shell.execute_reply.started":"2024-10-05T14:57:29.585115Z","shell.execute_reply":"2024-10-05T14:57:30.827401Z"},"trusted":true},"execution_count":13,"outputs":[{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/483 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"192813a2fa45441b8a4f1ef019cb17e8"}},"metadata":{}}]},{"cell_type":"code","source":"import os\n\n# list all files in the current directory\nfiles = os.listdir('.')\n\n# filter the ones that start with 'CORE_baseline'\ncore_models = [f for f in files if f.startswith('CORE_ensemble(core + dischargebert) + distilBert')]\n\nif core_models:\n    print(\"Found models starting with 'CORE_ensemble(core + dischargebert) + distilBert':\")\n    for model in core_models:\n        print(model)\n        \n    # get the first (and supposedly only) model\n    model_path = core_models[0]\n\n    # load the model state\n    student_model.load_state_dict(torch.load(model_path))\n    print(f\"Loaded Model: {model_path}\")\nelse:\n    print(\"No models found starting with 'CORE_ensemble(core + dischargebert) + distilBert'.\")","metadata":{"execution":{"iopub.status.busy":"2024-10-05T14:57:30.829623Z","iopub.execute_input":"2024-10-05T14:57:30.829935Z","iopub.status.idle":"2024-10-05T14:57:30.837408Z","shell.execute_reply.started":"2024-10-05T14:57:30.829903Z","shell.execute_reply":"2024-10-05T14:57:30.836510Z"},"trusted":true},"execution_count":14,"outputs":[{"name":"stdout","text":"No models found starting with 'CORE_ensemble(core + dischargebert) + distilBert'.\n","output_type":"stream"}]},{"cell_type":"code","source":"# Push the model to device\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nstudent_model = student_model.to(device)","metadata":{"execution":{"iopub.status.busy":"2024-10-05T14:57:30.838604Z","iopub.execute_input":"2024-10-05T14:57:30.838963Z","iopub.status.idle":"2024-10-05T14:57:30.923147Z","shell.execute_reply.started":"2024-10-05T14:57:30.838917Z","shell.execute_reply":"2024-10-05T14:57:30.922420Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\nval_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\ntest_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)","metadata":{"execution":{"iopub.status.busy":"2024-10-05T14:57:30.924098Z","iopub.execute_input":"2024-10-05T14:57:30.924377Z","iopub.status.idle":"2024-10-05T14:57:30.928943Z","shell.execute_reply.started":"2024-10-05T14:57:30.924346Z","shell.execute_reply":"2024-10-05T14:57:30.928135Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"epochs = 200\nbest_roc_auc = 0.0\nmin_delta = 0.0001\nearly_stopping_count = 0\nearly_stopping_patience = 3\ngradient_accumulation_steps = 10\n\n# Set the optimizer\noptimizer = AdamW(student_model.parameters(), lr=1e-5, weight_decay=0.01)\n\n# Set the scheduler\nscheduler = get_linear_schedule_with_warmup(\n    optimizer, \n    num_warmup_steps=50, \n    num_training_steps=len(train_loader) * epochs // gradient_accumulation_steps\n)","metadata":{"execution":{"iopub.status.busy":"2024-10-05T14:57:30.930105Z","iopub.execute_input":"2024-10-05T14:57:30.930395Z","iopub.status.idle":"2024-10-05T14:57:31.346497Z","shell.execute_reply.started":"2024-10-05T14:57:30.930365Z","shell.execute_reply":"2024-10-05T14:57:31.345697Z"},"trusted":true},"execution_count":17,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n  warnings.warn(\n","output_type":"stream"}]},{"cell_type":"code","source":"from torch.nn import functional as F\n\nensemble_model.eval()\n\n# Training\nfor epoch in range(epochs):\n    student_model.train()\n    train_loss = 0\n    for step, batch in enumerate(tqdm(train_loader)):\n        optimizer.zero_grad() if step % gradient_accumulation_steps == 0 else None\n        input_ids = batch['input_ids'].to(device)\n        attention_mask = batch['attention_mask'].to(device)\n        labels = batch['labels'].to(device)\n        \n        # get student model's logits\n        student_logits = student_model(input_ids, attention_mask)[0]\n        \n        # get teacher model's logits\n        with torch.no_grad():\n            teacher_logits = ensemble_model(input_ids, attention_mask)\n            \n            \n        # calculate loss\n        loss = (\n            nn.KLDivLoss()(F.log_softmax(student_logits/temperature, dim=1), \n                           F.softmax(teacher_logits/temperature, dim=1)) * (temperature ** 2) +\n            nn.CrossEntropyLoss()(student_logits, labels)\n        )\n        \n        (loss / gradient_accumulation_steps).backward()\n        \n        train_loss += loss.item()\n        \n        if (step + 1) % gradient_accumulation_steps == 0 or (step + 1) == len(train_loader):\n            optimizer.step()\n            scheduler.step()\n\n    student_model.eval()\n    val_loss = 0\n    val_preds = []\n    val_labels = []\n    with torch.no_grad():\n        for batch in tqdm(val_loader):\n            input_ids = batch['input_ids'].to(device)\n            attention_mask = batch['attention_mask'].to(device)\n            labels = batch['labels'].to(device)\n            outputs = student_model(input_ids, attention_mask)[0]\n            loss = nn.CrossEntropyLoss()(outputs, labels)\n            val_loss += loss.item()\n            val_preds.append(F.softmax(outputs, dim=1).cpu().numpy())\n            val_labels.append(labels.cpu().numpy())\n            \n\n    val_preds = np.concatenate(val_preds)\n    val_labels = np.concatenate(val_labels)\n    val_loss /= len(val_loader)\n    train_loss /= len(train_loader)\n    print(f'Epoch: {epoch+1}/{epochs}, Training Loss: {train_loss}, Validation Loss: {val_loss}')\n\n    # Calculate metrics\n    val_preds_class = np.argmax(val_preds, axis=1)\n    accuracy = accuracy_score(val_labels, val_preds_class)\n    recall = recall_score(val_labels, val_preds_class, average='weighted')\n    precision = precision_score(val_labels, val_preds_class, average='weighted')\n    f1 = f1_score(val_labels, val_preds_class, average='weighted')\n    micro_f1 = f1_score(val_labels, val_preds_class, average='micro')\n    macro_roc_auc = roc_auc_score(val_labels, val_preds, multi_class='ovo', average='macro')\n\n    print(f'Accuracy: {accuracy}, Recall: {recall}, Precision: {precision}, F1: {f1}, Micro F1: {micro_f1}, Macro Roc Auc: {macro_roc_auc}')\n        \n        \n    # Implement early stopping\n    if macro_roc_auc - best_roc_auc < min_delta:\n        early_stopping_count += 1\n        print(f'EarlyStopping counter: {early_stopping_count} out of {early_stopping_patience}')\n        if early_stopping_count >= early_stopping_patience:\n            print('Early stopping')\n            break\n    else:\n        best_roc_auc = macro_roc_auc\n        early_stopping_count = 0\n        torch.save(student_model.state_dict(), f\"CORE_ensemble(core + dischargebert) + distilBert_epoch_{epoch}roc_{best_roc_auc}.pth\")","metadata":{"execution":{"iopub.status.busy":"2024-10-05T14:57:31.347707Z","iopub.execute_input":"2024-10-05T14:57:31.348003Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stderr","text":"  0%|          | 0/951 [00:00<?, ?it/s]/opt/conda/lib/python3.10/site-packages/torch/nn/functional.py:2994: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.\n  warnings.warn(\n100%|██████████| 951/951 [54:07<00:00,  3.42s/it]\n100%|██████████| 138/138 [01:18<00:00,  1.77it/s]\n/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n","output_type":"stream"},{"name":"stdout","text":"Epoch: 1/200, Training Loss: 1.3555743918684628, Validation Loss: 1.318701389907063\nAccuracy: 0.3664313368253245, Recall: 0.3664313368253245, Precision: 0.13427192460759443, F1: 0.19652934031731573, Micro F1: 0.3664313368253245, Macro Roc Auc: 0.5690790489829739\n","output_type":"stream"},{"name":"stderr","text":"  0%|          | 0/951 [00:00<?, ?it/s]/opt/conda/lib/python3.10/site-packages/torch/nn/functional.py:2994: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.\n  warnings.warn(\n100%|██████████| 951/951 [54:17<00:00,  3.43s/it]\n100%|██████████| 138/138 [01:18<00:00,  1.77it/s]\n/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n","output_type":"stream"},{"name":"stdout","text":"Epoch: 2/200, Training Loss: 1.3353964427542862, Validation Loss: 1.310847464678944\nAccuracy: 0.36620359826918697, Recall: 0.36620359826918697, Precision: 0.1686930651444182, F1: 0.19703239660987715, Micro F1: 0.36620359826918697, Macro Roc Auc: 0.5755925533807893\n","output_type":"stream"},{"name":"stderr","text":"  0%|          | 0/951 [00:00<?, ?it/s]/opt/conda/lib/python3.10/site-packages/torch/nn/functional.py:2994: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.\n  warnings.warn(\n100%|██████████| 951/951 [53:09<00:00,  3.35s/it]\n100%|██████████| 138/138 [01:15<00:00,  1.82it/s]\n/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n","output_type":"stream"},{"name":"stdout","text":"Epoch: 3/200, Training Loss: 1.3298935608156848, Validation Loss: 1.3044652290966199\nAccuracy: 0.37075836939193807, Recall: 0.37075836939193807, Precision: 0.3189848005860356, F1: 0.22449062797954236, Micro F1: 0.37075836939193807, Macro Roc Auc: 0.5851857055080648\n","output_type":"stream"},{"name":"stderr","text":"  0%|          | 0/951 [00:00<?, ?it/s]/opt/conda/lib/python3.10/site-packages/torch/nn/functional.py:2994: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.\n  warnings.warn(\n100%|██████████| 951/951 [53:07<00:00,  3.35s/it]\n100%|██████████| 138/138 [01:18<00:00,  1.76it/s]\n/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n","output_type":"stream"},{"name":"stdout","text":"Epoch: 4/200, Training Loss: 1.3183366018890958, Validation Loss: 1.2880256754764612\nAccuracy: 0.3718970621726258, Recall: 0.3718970621726258, Precision: 0.3175348679484525, F1: 0.23428474659484555, Micro F1: 0.3718970621726258, Macro Roc Auc: 0.6190049110745085\n","output_type":"stream"},{"name":"stderr","text":"  0%|          | 0/951 [00:00<?, ?it/s]/opt/conda/lib/python3.10/site-packages/torch/nn/functional.py:2994: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.\n  warnings.warn(\n100%|██████████| 951/951 [54:18<00:00,  3.43s/it]\n100%|██████████| 138/138 [01:18<00:00,  1.77it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch: 5/200, Training Loss: 1.2999373468816218, Validation Loss: 1.2816660896591519\nAccuracy: 0.38032338874971533, Recall: 0.38032338874971533, Precision: 0.35778176724563576, F1: 0.25682015530738106, Micro F1: 0.38032338874971533, Macro Roc Auc: 0.6425371786168218\n","output_type":"stream"},{"name":"stderr","text":"  0%|          | 0/951 [00:00<?, ?it/s]/opt/conda/lib/python3.10/site-packages/torch/nn/functional.py:2994: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.\n  warnings.warn(\n100%|██████████| 951/951 [54:14<00:00,  3.42s/it]\n100%|██████████| 138/138 [01:18<00:00,  1.77it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch: 6/200, Training Loss: 1.2788588170874884, Validation Loss: 1.2636558111163154\nAccuracy: 0.39945342746526985, Recall: 0.39945342746526985, Precision: 0.4067761594396947, F1: 0.32238910705093904, Micro F1: 0.39945342746526985, Macro Roc Auc: 0.6611914350786644\n","output_type":"stream"},{"name":"stderr","text":"  0%|          | 0/951 [00:00<?, ?it/s]/opt/conda/lib/python3.10/site-packages/torch/nn/functional.py:2994: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.\n  warnings.warn(\n100%|██████████| 951/951 [54:16<00:00,  3.42s/it]\n100%|██████████| 138/138 [01:18<00:00,  1.76it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch: 7/200, Training Loss: 1.2640550057594708, Validation Loss: 1.2348452478215315\nAccuracy: 0.41197904805283536, Recall: 0.41197904805283536, Precision: 0.41188043209336167, F1: 0.38314304759323126, Micro F1: 0.41197904805283536, Macro Roc Auc: 0.6702612737990458\n","output_type":"stream"},{"name":"stderr","text":"  0%|          | 0/951 [00:00<?, ?it/s]/opt/conda/lib/python3.10/site-packages/torch/nn/functional.py:2994: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.\n  warnings.warn(\n100%|██████████| 951/951 [53:48<00:00,  3.40s/it]\n100%|██████████| 138/138 [01:15<00:00,  1.82it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch: 8/200, Training Loss: 1.2518458833955188, Validation Loss: 1.2248974166054656\nAccuracy: 0.41266226372124803, Recall: 0.41266226372124803, Precision: 0.4103806323219641, F1: 0.3646068414213535, Micro F1: 0.41266226372124803, Macro Roc Auc: 0.6795974797917869\n","output_type":"stream"},{"name":"stderr","text":"  0%|          | 0/951 [00:00<?, ?it/s]/opt/conda/lib/python3.10/site-packages/torch/nn/functional.py:2994: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.\n  warnings.warn(\n100%|██████████| 951/951 [52:51<00:00,  3.33s/it]\n100%|██████████| 138/138 [01:15<00:00,  1.82it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch: 9/200, Training Loss: 1.2393395997246983, Validation Loss: 1.217218892297883\nAccuracy: 0.42245502163516285, Recall: 0.42245502163516285, Precision: 0.4240741115166495, F1: 0.39660189237256815, Micro F1: 0.42245502163516285, Macro Roc Auc: 0.6837741575298314\n","output_type":"stream"},{"name":"stderr","text":"  0%|          | 0/951 [00:00<?, ?it/s]/opt/conda/lib/python3.10/site-packages/torch/nn/functional.py:2994: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.\n  warnings.warn(\n100%|██████████| 951/951 [52:54<00:00,  3.34s/it]\n100%|██████████| 138/138 [01:15<00:00,  1.82it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch: 10/200, Training Loss: 1.2320249207262988, Validation Loss: 1.2099261404811472\nAccuracy: 0.42086085174219995, Recall: 0.42086085174219995, Precision: 0.4303125423023369, F1: 0.39854691210393617, Micro F1: 0.42086085174219995, Macro Roc Auc: 0.6856816287281174\n","output_type":"stream"},{"name":"stderr","text":"  0%|          | 0/951 [00:00<?, ?it/s]/opt/conda/lib/python3.10/site-packages/torch/nn/functional.py:2994: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.\n  warnings.warn(\n100%|██████████| 951/951 [53:08<00:00,  3.35s/it]\n100%|██████████| 138/138 [01:17<00:00,  1.77it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch: 11/200, Training Loss: 1.2242057133548268, Validation Loss: 1.215441294338392\nAccuracy: 0.43179230243680256, Recall: 0.43179230243680256, Precision: 0.42939519935529147, F1: 0.407436909805495, Micro F1: 0.43179230243680256, Macro Roc Auc: 0.6900742278328665\n","output_type":"stream"},{"name":"stderr","text":"  0%|          | 0/951 [00:00<?, ?it/s]/opt/conda/lib/python3.10/site-packages/torch/nn/functional.py:2994: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.\n  warnings.warn(\n100%|██████████| 951/951 [54:14<00:00,  3.42s/it]\n100%|██████████| 138/138 [01:18<00:00,  1.76it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch: 12/200, Training Loss: 1.2166348193095435, Validation Loss: 1.2273018584735151\nAccuracy: 0.42040537462992483, Recall: 0.42040537462992483, Precision: 0.4233822460583641, F1: 0.3789734722570503, Micro F1: 0.42040537462992483, Macro Roc Auc: 0.6913962898590272\n","output_type":"stream"},{"name":"stderr","text":"  0%|          | 0/951 [00:00<?, ?it/s]/opt/conda/lib/python3.10/site-packages/torch/nn/functional.py:2994: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.\n  warnings.warn(\n100%|██████████| 951/951 [54:14<00:00,  3.42s/it]\n100%|██████████| 138/138 [01:18<00:00,  1.77it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch: 13/200, Training Loss: 1.2114101922850504, Validation Loss: 1.2135058477304983\nAccuracy: 0.4210885902983375, Recall: 0.4210885902983375, Precision: 0.42988761175386264, F1: 0.40385584574141564, Micro F1: 0.4210885902983375, Macro Roc Auc: 0.6895242656153174\nEarlyStopping counter: 1 out of 3\n","output_type":"stream"},{"name":"stderr","text":"  0%|          | 0/951 [00:00<?, ?it/s]/opt/conda/lib/python3.10/site-packages/torch/nn/functional.py:2994: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.\n  warnings.warn(\n  5%|▌         | 50/951 [02:51<51:24,  3.42s/it]","output_type":"stream"}]},{"cell_type":"code","source":"import os\n\n# list all files in the current directory\nfiles = os.listdir('.')\n\n# filter the ones that start with 'CORE_baseline'\ncore_models = [f for f in files if f.startswith('CORE_ensemble(core + dischargebert) + distilBert')]\n\nif core_models:\n    print(\"Found models starting with 'CORE_ensemble(core + dischargebert) + distilBert':\")\n    for model in core_models:\n        print(model)\n        \n    # get the first (and supposedly only) model\n    model_path = core_models[0]\n\n    # load the model state\n    student_model.load_state_dict(torch.load(model_path))\n    print(\"Loaded Model\")\nelse:\n    print(\"No models found starting with 'CORE_ensemble(core + dischargebert) + distilBert'.\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Put the model in evaluation mode\nstudent_model.eval()\n\n# Initialize lists to store predictions and true labels\ntest_preds = []\ntest_labels = []\n\n# Iterate over test data\nwith torch.no_grad():\n    for batch in tqdm(test_loader):\n        input_ids = batch['input_ids'].to(device)\n        attention_mask = batch['attention_mask'].to(device)\n        labels = batch['labels'].to(device)\n        outputs = student_model(input_ids, attention_mask)[0]\n        test_preds.append(F.softmax(outputs, dim=1).cpu().numpy())\n        test_labels.append(labels.cpu().numpy())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_preds = np.concatenate(test_preds)\ntest_labels = np.concatenate(test_labels)\n\n# Calculate metrics\ntest_preds_class = np.argmax(test_preds, axis=1)\naccuracy = accuracy_score(test_labels, test_preds_class)\nrecall = recall_score(test_labels, test_preds_class, average='weighted')\nprecision = precision_score(test_labels, test_preds_class, average='weighted')\nf1 = f1_score(test_labels, test_preds_class, average='weighted')\nmicro_f1 = f1_score(test_labels, test_preds_class, average='micro')\nmacro_roc_auc = roc_auc_score(test_labels, test_preds, multi_class='ovo', average='macro')\n\nprint(f'Accuracy: {accuracy}, Recall: {recall}, Precision: {precision}, F1: {f1}, Micro F1: {micro_f1}, Macro Roc Auc: {macro_roc_auc}')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}